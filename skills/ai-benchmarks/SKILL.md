---
name: ai-benchmarks
description: Skill for ai-benchmarks tasks and workflows.
---

# Skill: AI Benchmarks 2026 (v1.0)

> Beyond traditional LLM benchmarks

## Purpose
Understand modern AI evaluation landscape.

## Activation Trigger
- Model comparison needs
- Performance evaluation
- Capability assessment

---

## Benchmark Categories

### 1. Multimodal
| Benchmark | Focus |
|:---|:---|
| MMMU | College-level multimodal reasoning |
| HELM Safety | Safety evaluation |
| LM Arena Vision | Human preference (visual) |
| ScreenSpot-Pro | GUI grounding |

### 2. Reasoning
| Benchmark | Focus |
|:---|:---|
| HLE | Expert-level, no memorization |
| GPQA | PhD-level science questions |
| GSM8K | Grade school math reasoning |
| MATH | Competition math problems |
| FrontierMath | Research-level math |

### 3. Real-World
| Benchmark | Focus |
|:---|:---|
| SWE-bench | GitHub issue resolution |
| WebArena | Shopping agent tasks |
| PaperBench | Scientific research |
| Multi-IF | Multi-turn multilingual |

### 4. Safety & Ethics
| Benchmark | Focus |
|:---|:---|
| AgentHarm | Guardrail override tests |
| ST-WebAgentBench | High-risk business apps |

---

## 2026 Leaders

| Benchmark | Top Model | Score |
|:---|:---|:---:|
| SWE-bench Verified | Claude Opus 4.5 | 80.9% |
| GPQA | GPT-5.2 | High |
| HLE | Claude/GPT | Low (hard) |
| SWE-bench Pro | GPT-5 | 23.3% |

---

## For Sovereign Framework

Benchmarks inform:
1. **Model selection**: Choose best for task
2. **Capability limits**: Know what AI can't do
3. **Safety validation**: Test guardrails
4. **Progress tracking**: Measure improvement

